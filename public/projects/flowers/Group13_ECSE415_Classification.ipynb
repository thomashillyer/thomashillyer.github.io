{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import itertools\n",
    "from joblib import dump, load\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Assuming folder structure:\n",
    "# ./\n",
    "#  ^---this doc\n",
    "#  ^---flower_classification/\n",
    "#                          ^---testing/\n",
    "#                          ^---training/\n",
    "#                          ^---training_label.csv\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_set(dir_path, gray=False):\n",
    "    # Return a list of tuples\n",
    "    # [(image number, image), ...]\n",
    "    x = []\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if(filename.endswith(\".jpg\")):\n",
    "            img_num = int(filename.split('_')[1].split('.')[0])\n",
    "            if(gray):\n",
    "                x.append((img_num, cv2.imread(os.path.join(dir_path, filename))))\n",
    "            else:\n",
    "                x.append((img_num, cv2.imread(os.path.join(dir_path, filename))))\n",
    "                \n",
    "    return x\n",
    "\n",
    "def load_training_labels(label_path):\n",
    "    # Return a list of tuples\n",
    "    # [(image number, category), ...]\n",
    "    labels = []\n",
    "    with open(label_path, newline='\\n') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for line in reader:\n",
    "            labels.append((int(line['Id'].split('_')[1]), int(line['Category'])))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "train_images = load_set('./flower_classification/training/')\n",
    "train_labels = load_training_labels('./flower_classification/training_label.csv')\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort training images by Image Number\n",
    "train_images.sort(key=lambda x: int(x[0]))\n",
    "train_labels.sort(key=lambda x: int(x[0]))\n",
    "\n",
    "# Ensure data and labels are correct\n",
    "for i in range(len(train_images)):\n",
    "    if(train_images[i][0] != train_labels[i][0]):\n",
    "        print(\"Image numbers are not aligned!\")\n",
    "if(len(train_images) != len(train_labels)):\n",
    "    print(\"Different number of images & labels!\")\n",
    "\n",
    "# Remove image number from arrays\n",
    "train_images = np.array([x[1] for x in train_images])\n",
    "train_labels = np.array([x[1] for x in train_labels])\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(image_set, dim=(500,500)):\n",
    "    '''\n",
    "        Helper function to resize all the training images in the dataset - was used for testing with HOG's\n",
    "    '''\n",
    "    for i, img in enumerate(image_set):\n",
    "        image_set[i] = cv2.resize(img, dim)\n",
    "\n",
    "# resize_images(train_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect top n SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ SIFT ################\n",
    "'''\n",
    "    Create a SIFT object\n",
    "    Clamp the number of n-best keypoints returned, this is to keep the dimensionality of our input to k-means clustering\n",
    "    reasonably sized to avoid immense run times.\n",
    "'''\n",
    "# \n",
    "# \n",
    "def get_sift_features(data):\n",
    "    features = []\n",
    "    for img in data:\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures=20)\n",
    "\n",
    "        # detect SIFT features for image\n",
    "        keypoints = sift.detect(img, None)\n",
    "        keypoints, descriptors = sift.compute(img, keypoints)\n",
    "        features.append((np.array(keypoints), np.array(descriptors)))\n",
    "    return features\n",
    "\n",
    "\n",
    "sift_feats = get_sift_features(train_images)\n",
    "\n",
    "##################################\n",
    "\n",
    "\n",
    "\n",
    "############# HOG #################\n",
    "'''\n",
    "Similar compute hog function to that seen in our tutorials.\n",
    "\n",
    "Changed the reshaping to leave it as the full n-dim vector and made it function over a list of images\n",
    "'''\n",
    "\n",
    "def get_hog_features(data):\n",
    "    cell_size=(8,8)\n",
    "    block_size=(2,2)\n",
    "    \n",
    "    def compute_hog(train, cell_size=cell_size, block_size=block_size, nbins=9):\n",
    "        feature_maps = []\n",
    "        for i, img in enumerate(train):\n",
    "            hog = cv2.HOGDescriptor(_winSize=(img.shape[1], img.shape[0]),\n",
    "                                    _blockSize=(block_size[1] * cell_size[1],\n",
    "                                                block_size[0] * cell_size[0]),\n",
    "                                    _blockStride=(cell_size[1], cell_size[0]),\n",
    "                                    _cellSize=(cell_size[1], cell_size[0]),\n",
    "                                    _nbins=nbins)\n",
    "\n",
    "            n_cells = (img.shape[0] // cell_size[0], img.shape[1] // cell_size[1])\n",
    "\n",
    "            # Compute HoG features\n",
    "            hog_feats = hog.compute(img)\n",
    "\n",
    "            # Add the HOG vector to the list\n",
    "            feature_maps.append([i, hog_feats])\n",
    "        return feature_maps\n",
    "\n",
    "##########################################\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the keypoints and descriptors\n",
    "sift_kps = np.array([x[0] for x in sift_feats])\n",
    "sift_descs = np.array([x[1] for x in sift_feats])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sift features: 6000\n",
      "number of keypoints per image: 21\n",
      "number of descriptors per image: 21\n",
      "size of descriptor: 128\n",
      "sift_descs shape (6000,) (21, 128)\n"
     ]
    }
   ],
   "source": [
    "# To perform KMeans clustering, our data must maintain the same dimensionality\n",
    "# Clamp or expand to 0 vectors depending on number of n-best SIFT features used\n",
    "for i, desc in enumerate(sift_descs):\n",
    "    if(len(desc) > 20):\n",
    "        sift_descs[i] = desc[:20]\n",
    "    if(len(desc) < 20):\n",
    "        # pad with 0 vectors if couldnt discover 20 keypoints\n",
    "        for j in range(20 - len(desc)):\n",
    "            sift_descs[i] = np.append(sift_descs[i], np.zeros((1,128)), axis=0)\n",
    "            \n",
    "# Stack our input -- functionally a reshape\n",
    "sift_descs = np.stack(sift_descs, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform KMeans Clustering on full set of SIFT descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input: (6000, 20, 128)\n",
      "Reshaped for clustering: (120000, 128)\n",
      "Fitting on k=300 clusters...\n",
      "Predicting...\n",
      "Finished feature extraction.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def bag_of_words(X, n_clusters):\n",
    "    '''\n",
    "        Create visual vocabulary by clustering SIFT descriptors across whole dataset\n",
    "        n_clusters = n-visual words\n",
    "    '''\n",
    "    X_shape = X.shape\n",
    "    print(\"Shape of input: {}\".format(X.shape))\n",
    "    X = np.reshape(X, (X.shape[0] * X.shape[1], 128))\n",
    "    print(\"Reshaped for clustering: {}\".format(X.shape))\n",
    "    \n",
    "    print(\"Fitting on k={} clusters...\".format(n_clusters))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    \n",
    "    # Restore shape\n",
    "    X = np.reshape(X, X_shape)\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "    cluster_hists = np.zeros((X.shape[0], n_clusters)) \n",
    "    for i, x in enumerate(X):\n",
    "        # x is data point with 20 SIFT descriptors\n",
    "        \n",
    "        # find which cluster each SIFT descriptor is closest to\n",
    "        preds = kmeans.predict(x)\n",
    "        \n",
    "        # resultant vector (n-cluster dim histogram) is feature vector for data point x\n",
    "        bc = np.bincount(preds, minlength=n_clusters)\n",
    "        # normalize vector to avoid massive weight discrepancies\n",
    "        cluster_hists[i] = bc / np.linalg.norm(bc)\n",
    "        \n",
    "    # Other option for vector normalization is to encode as 1-hot bag of words\n",
    "#     for i in range(len(cluster_hists)):\n",
    "#         for j in range(len(cluster_hists[i])):\n",
    "#             cluster_hists[i][j] = 1 if cluster_hists[i][j] > 0 else 0\n",
    "    \n",
    "    return cluster_hists\n",
    "\n",
    "new_X = bag_of_words(sift_descs, 300)\n",
    "\n",
    "print(\"Finished feature extraction.\")\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train x and y: (4800, 300) and (4800,)\n",
      "Shape of test x and y: (1200, 300) and (1200,)\n",
      "Classification report for [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       1.00      0.20      0.33         5\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.00      0.00      0.00         7\n",
      "           8       0.17      0.45      0.25        11\n",
      "           9       0.00      0.00      0.00         7\n",
      "          10       0.00      0.00      0.00         6\n",
      "          11       0.00      0.00      0.00        15\n",
      "          12       0.57      0.31      0.40        13\n",
      "          13       0.00      0.00      0.00         6\n",
      "          14       0.00      0.00      0.00        10\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         8\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00        16\n",
      "          19       0.00      0.00      0.00         9\n",
      "          20       0.00      0.00      0.00         7\n",
      "          21       0.00      0.00      0.00         9\n",
      "          22       0.00      0.00      0.00        10\n",
      "          23       1.00      0.06      0.11        17\n",
      "          24       0.00      0.00      0.00         9\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.00      0.00      0.00         3\n",
      "          27       0.00      0.00      0.00         5\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00        11\n",
      "          30       0.00      0.00      0.00         8\n",
      "          31       0.00      0.00      0.00        11\n",
      "          32       0.00      0.00      0.00         9\n",
      "          33       0.00      0.00      0.00         7\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00         7\n",
      "          37       0.11      0.12      0.12        16\n",
      "          38       0.00      0.00      0.00         5\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00         8\n",
      "          41       0.07      0.08      0.08        12\n",
      "          42       0.00      0.00      0.00         5\n",
      "          43       0.00      0.00      0.00        24\n",
      "          44       0.00      0.00      0.00        13\n",
      "          45       0.00      0.00      0.00         9\n",
      "          46       0.13      0.32      0.18        34\n",
      "          47       0.00      0.00      0.00        11\n",
      "          48       0.25      0.11      0.15         9\n",
      "          49       0.00      0.00      0.00         6\n",
      "          50       0.39      0.47      0.42        15\n",
      "          51       0.12      0.58      0.20        45\n",
      "          52       0.36      0.67      0.47        12\n",
      "          53       0.00      0.00      0.00        10\n",
      "          54       0.00      0.00      0.00         9\n",
      "          55       0.00      0.00      0.00        10\n",
      "          56       0.00      0.00      0.00        19\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.50      0.16      0.24        19\n",
      "          59       0.00      0.00      0.00         6\n",
      "          60       0.00      0.00      0.00        13\n",
      "          61       0.00      0.00      0.00         6\n",
      "          62       0.00      0.00      0.00        15\n",
      "          63       0.43      0.43      0.43         7\n",
      "          64       0.50      0.33      0.40         3\n",
      "          65       0.20      0.10      0.13        10\n",
      "          66       0.67      0.17      0.27        12\n",
      "          67       0.00      0.00      0.00         5\n",
      "          68       0.00      0.00      0.00         8\n",
      "          69       0.00      0.00      0.00         2\n",
      "          70       0.00      0.00      0.00         9\n",
      "          71       0.00      0.00      0.00        17\n",
      "          72       0.00      0.00      0.00        16\n",
      "          73       0.28      0.60      0.38        43\n",
      "          74       0.11      0.26      0.16        23\n",
      "          75       0.50      0.41      0.45        17\n",
      "          76       0.10      0.10      0.10        10\n",
      "          77       0.09      0.84      0.16        32\n",
      "          78       0.25      0.25      0.25        24\n",
      "          79       0.00      0.00      0.00         8\n",
      "          80       0.42      0.25      0.31        20\n",
      "          81       0.08      0.27      0.13        22\n",
      "          82       0.00      0.00      0.00        16\n",
      "          83       0.00      0.00      0.00        17\n",
      "          84       0.00      0.00      0.00        19\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.00      0.00      0.00         7\n",
      "          87       0.00      0.00      0.00        11\n",
      "          88       0.08      0.07      0.07        15\n",
      "          89       0.12      0.28      0.17        29\n",
      "          90       0.00      0.00      0.00         8\n",
      "          91       0.00      0.00      0.00         8\n",
      "          92       0.00      0.00      0.00         8\n",
      "          93       0.00      0.00      0.00         6\n",
      "          94       0.14      0.61      0.22        18\n",
      "          95       0.00      0.00      0.00        15\n",
      "          96       0.00      0.00      0.00        11\n",
      "          97       0.00      0.00      0.00         8\n",
      "          98       0.00      0.00      0.00        11\n",
      "          99       0.00      0.00      0.00         6\n",
      "         100       0.00      0.00      0.00        10\n",
      "         101       0.00      0.00      0.00        12\n",
      "         102       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.15      1200\n",
      "   macro avg       0.08      0.08      0.06      1200\n",
      "weighted avg       0.11      0.15      0.10      1200\n",
      "\n",
      "\n",
      "Accuracy score: 0.15083333333333335\n",
      "\n",
      "Shape of train x and y: (4800, 300) and (4800,)\n",
      "Shape of test x and y: (1200, 300) and (1200,)\n",
      "Classification report for [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         5\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00        13\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         4\n",
      "           8       0.22      0.36      0.27        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00         9\n",
      "          12       0.43      0.50      0.46        12\n",
      "          13       0.00      0.00      0.00         7\n",
      "          14       0.00      0.00      0.00         6\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00        18\n",
      "          18       1.00      0.08      0.14        13\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00        13\n",
      "          21       0.00      0.00      0.00         3\n",
      "          22       0.00      0.00      0.00         8\n",
      "          23       0.00      0.00      0.00        14\n",
      "          24       0.00      0.00      0.00         7\n",
      "          25       0.00      0.00      0.00         4\n",
      "          26       0.00      0.00      0.00        12\n",
      "          27       0.00      0.00      0.00        11\n",
      "          28       0.00      0.00      0.00         6\n",
      "          29       0.00      0.00      0.00         5\n",
      "          30       0.00      0.00      0.00        19\n",
      "          31       0.00      0.00      0.00         3\n",
      "          32       0.00      0.00      0.00         6\n",
      "          33       0.00      0.00      0.00         9\n",
      "          34       0.00      0.00      0.00         6\n",
      "          35       0.00      0.00      0.00         5\n",
      "          36       0.00      0.00      0.00        17\n",
      "          37       0.40      0.26      0.32        23\n",
      "          38       0.00      0.00      0.00         9\n",
      "          39       0.00      0.00      0.00         8\n",
      "          40       0.00      0.00      0.00        11\n",
      "          41       0.14      0.05      0.08        19\n",
      "          42       0.00      0.00      0.00        13\n",
      "          43       0.00      0.00      0.00        15\n",
      "          44       0.00      0.00      0.00        13\n",
      "          45       0.00      0.00      0.00         5\n",
      "          46       0.10      0.37      0.16        27\n",
      "          47       0.00      0.00      0.00         7\n",
      "          48       0.50      0.10      0.17        10\n",
      "          49       0.00      0.00      0.00         8\n",
      "          50       0.21      0.25      0.23        12\n",
      "          51       0.08      0.50      0.13        34\n",
      "          52       0.32      0.64      0.42        11\n",
      "          53       0.00      0.00      0.00        12\n",
      "          54       0.00      0.00      0.00         6\n",
      "          55       0.00      0.00      0.00        14\n",
      "          56       0.05      0.09      0.06        11\n",
      "          57       0.00      0.00      0.00         8\n",
      "          58       0.00      0.00      0.00        17\n",
      "          59       0.00      0.00      0.00        10\n",
      "          60       0.00      0.00      0.00        10\n",
      "          61       0.00      0.00      0.00        11\n",
      "          62       0.00      0.00      0.00         6\n",
      "          63       0.50      0.11      0.18         9\n",
      "          64       0.75      0.43      0.55         7\n",
      "          65       0.67      0.13      0.22        15\n",
      "          66       0.40      0.57      0.47         7\n",
      "          67       0.00      0.00      0.00         9\n",
      "          68       0.00      0.00      0.00         9\n",
      "          69       0.00      0.00      0.00         5\n",
      "          70       0.00      0.00      0.00         6\n",
      "          71       0.00      0.00      0.00        11\n",
      "          72       0.00      0.00      0.00        12\n",
      "          73       0.14      0.50      0.22        32\n",
      "          74       0.09      0.23      0.13        22\n",
      "          75       0.26      0.41      0.32        17\n",
      "          76       0.00      0.00      0.00        22\n",
      "          77       0.10      0.73      0.18        41\n",
      "          78       0.21      0.26      0.24        23\n",
      "          79       0.00      0.00      0.00         3\n",
      "          80       0.14      0.14      0.14        21\n",
      "          81       0.10      0.12      0.11        25\n",
      "          82       0.00      0.00      0.00        17\n",
      "          83       0.00      0.00      0.00        21\n",
      "          84       0.00      0.00      0.00        16\n",
      "          85       0.00      0.00      0.00         6\n",
      "          86       0.00      0.00      0.00         4\n",
      "          87       0.00      0.00      0.00         5\n",
      "          88       0.00      0.00      0.00        21\n",
      "          89       0.09      0.29      0.13        21\n",
      "          90       0.00      0.00      0.00        13\n",
      "          91       0.00      0.00      0.00        16\n",
      "          92       0.00      0.00      0.00        13\n",
      "          93       0.00      0.00      0.00         4\n",
      "          94       0.33      0.80      0.47        25\n",
      "          95       0.00      0.00      0.00        15\n",
      "          96       0.00      0.00      0.00        16\n",
      "          97       0.00      0.00      0.00         7\n",
      "          98       0.00      0.00      0.00        14\n",
      "          99       0.00      0.00      0.00         8\n",
      "         100       0.00      0.00      0.00         4\n",
      "         101       0.00      0.00      0.00        16\n",
      "         102       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14      1200\n",
      "   macro avg       0.07      0.08      0.06      1200\n",
      "weighted avg       0.09      0.14      0.08      1200\n",
      "\n",
      "\n",
      "Accuracy score: 0.13666666666666666\n",
      "\n",
      "Shape of train x and y: (4800, 300) and (4800,)\n",
      "Shape of test x and y: (1200, 300) and (1200,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00         9\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.38      0.36      0.37        14\n",
      "           9       0.00      0.00      0.00        10\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       0.00      0.00      0.00        13\n",
      "          12       0.75      0.21      0.33        14\n",
      "          13       0.00      0.00      0.00         8\n",
      "          14       0.00      0.00      0.00         7\n",
      "          15       0.00      0.00      0.00         5\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00        13\n",
      "          18       0.00      0.00      0.00        14\n",
      "          19       0.00      0.00      0.00         5\n",
      "          20       0.00      0.00      0.00         7\n",
      "          21       0.00      0.00      0.00         7\n",
      "          22       0.00      0.00      0.00        10\n",
      "          23       0.00      0.00      0.00         7\n",
      "          24       0.00      0.00      0.00         3\n",
      "          25       0.00      0.00      0.00         5\n",
      "          26       0.00      0.00      0.00         5\n",
      "          27       0.00      0.00      0.00         5\n",
      "          28       0.00      0.00      0.00         7\n",
      "          29       0.00      0.00      0.00        10\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       0.00      0.00      0.00         9\n",
      "          32       0.00      0.00      0.00         5\n",
      "          33       0.00      0.00      0.00         2\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00        11\n",
      "          36       0.00      0.00      0.00        13\n",
      "          37       0.44      0.16      0.24        25\n",
      "          38       0.00      0.00      0.00         9\n",
      "          39       0.00      0.00      0.00         4\n",
      "          40       0.00      0.00      0.00        12\n",
      "          41       0.00      0.00      0.00        18\n",
      "          42       0.00      0.00      0.00         8\n",
      "          43       0.00      0.00      0.00        17\n",
      "          44       0.00      0.00      0.00        18\n",
      "          45       0.00      0.00      0.00         6\n",
      "          46       0.10      0.50      0.17        20\n",
      "          47       0.00      0.00      0.00        11\n",
      "          48       1.00      0.08      0.14        13\n",
      "          49       0.00      0.00      0.00        13\n",
      "          50       0.22      0.36      0.28        11\n",
      "          51       0.08      0.49      0.14        37\n",
      "          52       0.67      0.83      0.74        12\n",
      "          53       0.00      0.00      0.00        20\n",
      "          54       0.00      0.00      0.00         9\n",
      "          55       0.00      0.00      0.00        11\n",
      "          56       0.00      0.00      0.00        18\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.17      0.11      0.13        19\n",
      "          59       0.00      0.00      0.00        15\n",
      "          60       0.33      0.10      0.15        10\n",
      "          61       0.00      0.00      0.00        13\n",
      "          62       0.00      0.00      0.00         8\n",
      "          63       0.60      0.43      0.50         7\n",
      "          64       0.00      0.00      0.00        10\n",
      "          65       0.67      0.13      0.22        15\n",
      "          66       0.33      0.75      0.46         4\n",
      "          67       0.00      0.00      0.00         6\n",
      "          68       0.00      0.00      0.00        10\n",
      "          69       0.00      0.00      0.00         9\n",
      "          70       0.00      0.00      0.00         9\n",
      "          71       0.00      0.00      0.00        12\n",
      "          72       0.00      0.00      0.00        18\n",
      "          73       0.07      0.43      0.12        23\n",
      "          74       0.06      0.11      0.08        27\n",
      "          75       0.67      0.33      0.44        24\n",
      "          76       0.12      0.07      0.09        15\n",
      "          77       0.09      0.74      0.17        38\n",
      "          78       0.07      0.12      0.09        17\n",
      "          79       0.00      0.00      0.00         9\n",
      "          80       0.06      0.17      0.09        12\n",
      "          81       0.19      0.27      0.22        26\n",
      "          82       0.00      0.00      0.00        18\n",
      "          83       0.00      0.00      0.00        16\n",
      "          84       0.00      0.00      0.00        15\n",
      "          85       0.00      0.00      0.00         7\n",
      "          86       0.00      0.00      0.00         5\n",
      "          87       0.00      0.00      0.00        13\n",
      "          88       0.00      0.00      0.00        25\n",
      "          89       0.05      0.21      0.09        19\n",
      "          90       0.00      0.00      0.00        12\n",
      "          91       0.00      0.00      0.00        10\n",
      "          92       0.00      0.00      0.00         6\n",
      "          93       0.00      0.00      0.00         6\n",
      "          94       0.22      0.46      0.30        26\n",
      "          95       1.00      0.05      0.10        19\n",
      "          96       0.00      0.00      0.00        12\n",
      "          97       0.00      0.00      0.00        10\n",
      "          98       0.00      0.00      0.00         9\n",
      "          99       0.00      0.00      0.00         6\n",
      "         100       0.00      0.00      0.00        11\n",
      "         101       0.00      0.00      0.00         5\n",
      "         102       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.12      1200\n",
      "   macro avg       0.08      0.07      0.06      1200\n",
      "weighted avg       0.11      0.12      0.08      1200\n",
      "\n",
      "\n",
      "Accuracy score: 0.12\n",
      "\n",
      "Shape of train x and y: (4800, 300) and (4800,)\n",
      "Shape of test x and y: (1200, 300) and (1200,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthewc/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matthewc/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matthewc/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00         7\n",
      "           5       0.00      0.00      0.00        12\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.00      0.00      0.00         4\n",
      "           8       0.30      0.40      0.34        15\n",
      "           9       0.00      0.00      0.00         7\n",
      "          10       0.00      0.00      0.00         6\n",
      "          11       0.00      0.00      0.00        10\n",
      "          12       0.33      0.20      0.25        10\n",
      "          13       0.00      0.00      0.00         3\n",
      "          14       0.00      0.00      0.00         8\n",
      "          15       0.00      0.00      0.00        11\n",
      "          16       0.00      0.00      0.00         5\n",
      "          17       0.00      0.00      0.00         7\n",
      "          18       0.25      0.20      0.22        10\n",
      "          19       0.00      0.00      0.00        10\n",
      "          20       0.00      0.00      0.00        13\n",
      "          21       0.00      0.00      0.00         8\n",
      "          22       0.00      0.00      0.00         6\n",
      "          23       0.50      0.08      0.14        12\n",
      "          24       0.00      0.00      0.00         6\n",
      "          25       0.00      0.00      0.00         7\n",
      "          26       0.00      0.00      0.00         5\n",
      "          27       0.00      0.00      0.00         5\n",
      "          28       0.00      0.00      0.00        10\n",
      "          29       0.00      0.00      0.00        15\n",
      "          30       0.00      0.00      0.00        11\n",
      "          31       0.00      0.00      0.00         6\n",
      "          32       0.00      0.00      0.00         7\n",
      "          33       0.00      0.00      0.00         7\n",
      "          34       0.00      0.00      0.00         4\n",
      "          35       0.00      0.00      0.00         7\n",
      "          36       0.00      0.00      0.00        10\n",
      "          37       0.18      0.36      0.24        14\n",
      "          38       0.00      0.00      0.00         7\n",
      "          39       0.00      0.00      0.00         3\n",
      "          40       0.00      0.00      0.00         7\n",
      "          41       0.00      0.00      0.00        21\n",
      "          42       0.00      0.00      0.00         9\n",
      "          43       0.00      0.00      0.00        21\n",
      "          44       0.00      0.00      0.00        17\n",
      "          45       0.00      0.00      0.00         5\n",
      "          46       0.20      0.36      0.25        39\n",
      "          47       0.00      0.00      0.00        18\n",
      "          48       0.50      0.07      0.12        15\n",
      "          49       0.00      0.00      0.00         7\n",
      "          50       0.45      0.31      0.37        16\n",
      "          51       0.09      0.66      0.16        32\n",
      "          52       0.26      0.46      0.33        13\n",
      "          53       0.00      0.00      0.00        14\n",
      "          54       0.00      0.00      0.00        12\n",
      "          55       0.00      0.00      0.00         8\n",
      "          56       0.10      0.05      0.06        21\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.29      0.29      0.29        14\n",
      "          59       0.00      0.00      0.00        16\n",
      "          60       0.00      0.00      0.00        16\n",
      "          61       0.00      0.00      0.00         5\n",
      "          62       0.00      0.00      0.00         8\n",
      "          63       1.00      0.09      0.17        11\n",
      "          64       1.00      0.20      0.33        10\n",
      "          65       0.50      0.15      0.24        13\n",
      "          66       0.50      0.67      0.57         6\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       0.00      0.00      0.00         8\n",
      "          70       0.00      0.00      0.00         9\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       0.00      0.00      0.00        15\n",
      "          73       0.12      0.58      0.19        24\n",
      "          74       0.09      0.27      0.13        22\n",
      "          75       0.39      0.47      0.42        15\n",
      "          76       0.00      0.00      0.00        24\n",
      "          77       0.08      0.72      0.14        32\n",
      "          78       0.08      0.10      0.09        21\n",
      "          79       0.00      0.00      0.00         4\n",
      "          80       0.12      0.23      0.15        13\n",
      "          81       0.16      0.36      0.23        22\n",
      "          82       0.00      0.00      0.00        13\n",
      "          83       0.00      0.00      0.00        22\n",
      "          84       0.00      0.00      0.00        16\n",
      "          85       0.00      0.00      0.00        10\n",
      "          86       0.00      0.00      0.00        13\n",
      "          87       0.00      0.00      0.00         6\n",
      "          88       0.00      0.00      0.00        24\n",
      "          89       0.10      0.14      0.12        35\n",
      "          90       0.00      0.00      0.00        19\n",
      "          91       0.00      0.00      0.00         9\n",
      "          92       0.00      0.00      0.00         7\n",
      "          93       0.00      0.00      0.00         5\n",
      "          94       0.18      0.76      0.29        17\n",
      "          95       0.00      0.00      0.00        19\n",
      "          96       0.00      0.00      0.00        11\n",
      "          97       0.00      0.00      0.00        12\n",
      "          98       0.00      0.00      0.00         9\n",
      "          99       0.00      0.00      0.00         8\n",
      "         100       0.00      0.00      0.00         8\n",
      "         101       0.00      0.00      0.00         7\n",
      "         102       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.13      1200\n",
      "   macro avg       0.08      0.08      0.06      1200\n",
      "weighted avg       0.09      0.13      0.08      1200\n",
      "\n",
      "\n",
      "Accuracy score: 0.13166666666666665\n",
      "\n",
      "Shape of train x and y: (4800, 300) and (4800,)\n",
      "Shape of test x and y: (1200, 300) and (1200,)\n",
      "Classification report for [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00        11\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.00      0.00      0.00         9\n",
      "           7       0.00      0.00      0.00         7\n",
      "           8       0.11      0.57      0.19         7\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00         9\n",
      "          11       0.00      0.00      0.00        14\n",
      "          12       0.44      0.44      0.44         9\n",
      "          13       0.00      0.00      0.00        10\n",
      "          14       0.00      0.00      0.00         8\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00        14\n",
      "          18       0.00      0.00      0.00        13\n",
      "          19       0.00      0.00      0.00        11\n",
      "          20       0.00      0.00      0.00         7\n",
      "          21       0.00      0.00      0.00         4\n",
      "          22       0.00      0.00      0.00         9\n",
      "          23       0.00      0.00      0.00        17\n",
      "          24       0.00      0.00      0.00         7\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.00      0.00      0.00         6\n",
      "          27       0.00      0.00      0.00         7\n",
      "          28       0.00      0.00      0.00        12\n",
      "          29       0.00      0.00      0.00        12\n",
      "          30       0.00      0.00      0.00        10\n",
      "          31       0.00      0.00      0.00         9\n",
      "          32       0.00      0.00      0.00         7\n",
      "          33       0.00      0.00      0.00         8\n",
      "          34       0.00      0.00      0.00         7\n",
      "          35       0.00      0.00      0.00         8\n",
      "          36       0.00      0.00      0.00        10\n",
      "          37       0.04      0.08      0.05        12\n",
      "          38       0.00      0.00      0.00         5\n",
      "          39       0.00      0.00      0.00         8\n",
      "          40       0.00      0.00      0.00        11\n",
      "          41       0.17      0.05      0.08        20\n",
      "          42       0.00      0.00      0.00        12\n",
      "          43       0.00      0.00      0.00        14\n",
      "          44       0.00      0.00      0.00        11\n",
      "          45       0.00      0.00      0.00         7\n",
      "          46       0.12      0.44      0.19        25\n",
      "          47       1.00      0.20      0.33         5\n",
      "          48       0.67      0.20      0.31        10\n",
      "          49       0.00      0.00      0.00         6\n",
      "          50       0.38      0.31      0.34        16\n",
      "          51       0.14      0.70      0.23        46\n",
      "          52       0.38      0.82      0.51        11\n",
      "          53       0.00      0.00      0.00        12\n",
      "          54       0.00      0.00      0.00        12\n",
      "          55       0.00      0.00      0.00         9\n",
      "          56       0.07      0.17      0.10        12\n",
      "          57       0.00      0.00      0.00         8\n",
      "          58       0.17      0.12      0.14        16\n",
      "          59       0.00      0.00      0.00         5\n",
      "          60       0.00      0.00      0.00        23\n",
      "          61       0.00      0.00      0.00         5\n",
      "          62       0.00      0.00      0.00         6\n",
      "          63       1.00      0.29      0.44         7\n",
      "          64       1.00      0.40      0.57        10\n",
      "          65       0.00      0.00      0.00        18\n",
      "          66       1.00      0.43      0.60         7\n",
      "          67       0.00      0.00      0.00         9\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       0.00      0.00      0.00         8\n",
      "          70       0.00      0.00      0.00        12\n",
      "          71       0.00      0.00      0.00         5\n",
      "          72       0.00      0.00      0.00        11\n",
      "          73       0.10      0.54      0.17        26\n",
      "          74       0.07      0.10      0.08        30\n",
      "          75       0.27      0.35      0.31        17\n",
      "          76       0.00      0.00      0.00         7\n",
      "          77       0.11      0.86      0.19        35\n",
      "          78       0.22      0.20      0.21        30\n",
      "          79       0.00      0.00      0.00         4\n",
      "          80       0.13      0.12      0.12        17\n",
      "          81       0.18      0.19      0.18        32\n",
      "          82       0.00      0.00      0.00        14\n",
      "          83       0.00      0.00      0.00        23\n",
      "          84       1.00      0.11      0.20         9\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.00      0.00      0.00         7\n",
      "          87       0.00      0.00      0.00        10\n",
      "          88       0.00      0.00      0.00        18\n",
      "          89       0.33      0.39      0.36        33\n",
      "          90       0.00      0.00      0.00        10\n",
      "          91       0.00      0.00      0.00        10\n",
      "          92       0.00      0.00      0.00         5\n",
      "          93       0.00      0.00      0.00        15\n",
      "          94       0.33      0.78      0.46        27\n",
      "          95       1.00      0.07      0.13        14\n",
      "          96       0.00      0.00      0.00        16\n",
      "          97       0.00      0.00      0.00        11\n",
      "          98       0.00      0.00      0.00        13\n",
      "          99       0.00      0.00      0.00        13\n",
      "         100       0.00      0.00      0.00         7\n",
      "         101       0.00      0.00      0.00         6\n",
      "         102       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.15      1200\n",
      "   macro avg       0.10      0.09      0.07      1200\n",
      "weighted avg       0.12      0.15      0.10      1200\n",
      "\n",
      "\n",
      "Accuracy score: 0.155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthewc/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/matthewc/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "    The following method was taken from Scikit-learn's documentation\n",
    "'''\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def svm_classify_validation(X, y, max_iter=1500, C=0.1):\n",
    "    '''\n",
    "        Perform 5-fold cross validation using SVM\n",
    "    '''\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        print(\"Shape of train x and y: {} and {}\".format(X_train.shape, y_train.shape))\n",
    "        print(\"Shape of test x and y: {} and {}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "        classifier = svm.LinearSVC(max_iter=2000, C=0.15, random_state=0, tol=1e-5)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        print(\"Classification report for [%s]:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, y_pred)))\n",
    "        # Compute confusion matrix\n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "\n",
    "        # Plot non-normalized confusion matrix\n",
    "        plt.figure(figsize=(50,50))\n",
    "        plot_confusion_matrix(cnf_matrix, classes=[i for i in range(1, 103)],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "        plt.show()\n",
    "#         print(\"Accuracy score: %s\\n\" % metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "svm_classify_validation(new_X, train_labels)\n",
    "\n",
    "def mnb_classify_validation(X, y):\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        print(\"Shape of train x and y: {} and {}\".format(X_train.shape, y_train.shape))\n",
    "        print(\"Shape of test x and y: {} and {}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "        classifier = MultinomialNB()\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        print(\"Classification report for [%s]:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, y_pred)))\n",
    "        # Compute confusion matrix\n",
    "        cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "\n",
    "        # Plot non-normalized confusion matrix\n",
    "        plt.figure(figsize=(50,50))\n",
    "        plot_confusion_matrix(cnf_matrix, classes=[i for i in range(1, 103)],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "        plt.show()\n",
    "        print(\"Accuracy score: %s\\n\" % metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# mnb_classify_validation(new_X, train_labels)\n",
    "\n",
    "def save_model(X, y):\n",
    "    # Function to save model weights to be applied later -- good for saving memory\n",
    "    from joblib import dump, load\n",
    "    classifier = svm.LinearSVC(max_iter=2000, C=0.15, random_state=0, tol=1e-5)\n",
    "    classifier.fit(X, y)\n",
    "    \n",
    "    dump(classifier, './classifier.joblib')\n",
    "    return\n",
    "\n",
    "# save_model(new_X, train_labels)\n",
    "\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input: (2189, 20, 128)\n",
      "Reshaped for clustering: (43780, 128)\n",
      "Fitting on k=300 clusters...\n",
      "Predicting...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def classify_test_set():\n",
    "    # Load test set\n",
    "    test_images = load_set('./flower_classification/testing/')\n",
    "\n",
    "    # Split Image Number and Image\n",
    "    test_img_num = np.array([x[0] for x in test_images])\n",
    "    test_images = np.array([x[1] for x in test_images])\n",
    "\n",
    "    # Get sift features\n",
    "    test_sift_feats = get_sift_features(test_images)\n",
    "\n",
    "    # Separate the keypoints and descriptors\n",
    "    test_sift_kps = np.array([x[0] for x in test_sift_feats])\n",
    "    test_sift_descs = np.array([x[1] for x in test_sift_feats])\n",
    "\n",
    "    for i, desc in enumerate(test_sift_descs):\n",
    "        if(len(desc) > 20):\n",
    "            test_sift_descs[i] = desc[:20]\n",
    "        if(len(desc) < 20):\n",
    "            # pad with 0 vectors if couldnt discover 50 keypoints\n",
    "            for j in range(20 - len(desc)):\n",
    "                test_sift_descs[i] = np.append(test_sift_descs[i], np.zeros((1,128)), axis=0)\n",
    "\n",
    "    # Stack our input -- functionally a reshape\n",
    "    test_sift_descs = np.stack(test_sift_descs, 0)\n",
    "\n",
    "    X_test = bag_of_words(test_sift_descs, 300)\n",
    "\n",
    "    # Save classifier to use later on\n",
    "    classifier = load('classifier.joblib')\n",
    "    # classifier = svm.LinearSVC(max_iter=2000, C=0.15, random_state=0, tol=1e-5)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    allDone()\n",
    "\n",
    "# classify_test_set()\n",
    "\n",
    "# result = np.array(list(zip(img_numsss, y_pred)))\n",
    "# np.savetxt(\"results5.csv\", result, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
